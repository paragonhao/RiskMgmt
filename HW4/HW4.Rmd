---
header-includes:
- \usepackage{amssymb, amsmath, amsthm}
- \usepackage{tabu}
- \newcommand{\E}{\mathbb{E}}
- \newcommand{\var}{{\rm Var}}
- \newcommand{\N}{\mathcal{N}}
output: pdf_document
---

\noindent \begin{tabu} to \textwidth {@{}X[4 l] @{}X[r]}
  \textbf{HW 4}           & \\ 
  \textbf{MFE 409: Financial Risk Measurement and Management}   & \\ 
  \textbf{Professor Valentin Haddad}         & \\
  \textbf{Group 9}         & \\
  \textbf{Students: Xiahao Wang, Haoxuan Tong, Yuhua Deng, Nupur Solanki}
\end{tabu}


## Qn1

### 1.1

Calculate the Exponential Weighted Moving Average (EWMA)

```{r}
suppressMessages(require(data.table))
suppressMessages(require(lubridate))
suppressMessages(require(zoo))
suppressMessages(require(rugarch))

rm(list=ls())
returns_data <- as.data.table(read.csv("hw3_returns2.csv"))
returns_data[,Date:=mdy(Date)]

# get volatility
returns_data[, volatility := Return^2]
returns_data[, volatility := cumsum(volatility)]
returns_data[, observation := 1]
returns_data[, observation := cumsum(observation)]
returns_data[, volatility := volatility/observation]

lambda <- 0.94

returns_data[, one_minus_lambda := (1 - lambda)* (Return^2)]
returns_data$ewma[1] <- returns_data$one_minus_lambda[1]

size <- length(returns_data$ewma)

for(i in 2:size){
  returns_data$ewma[i] <- lambda * returns_data$ewma[i-1] +  returns_data$one_minus_lambda[i]
}

plot(x=returns_data$Date, y= returns_data$ewma, type="l", xlab = "Date", ylab="Volatility", main = "EWMA")
returns_data[, observation := NULL]
returns_data[, one_minus_lambda := NULL]
```
Next, find the VaR at 99% 
The daily Value at Risk (VaR) is simply a function of the standard deviation or volatility and the desired confidence level. 

Value at Risk (VAR) = sqrt(EWMA predicted Volatility) × z-value of standard normal cumulative distribution

Calculate Daily VaR for both historical and exponential

```{r}
c <- 0.01
z <- qnorm(0.01)
returns_data[, VaR := sqrt(ewma) * z]

data_2005_onwards <- returns_data[Date >= "2015-01-01"]

plot(x = data_2005_onwards$Date, y= data_2005_onwards$Return, type = "l", 
     main="Stock Return against VaRs", xlab = "Date",
     ylab= "Stock Return")
lines(x= data_2005_onwards$Date, y= data_2005_onwards$VaR, col = "blue")
legend("bottomright",legend=c("Return","VaR"),fill=c("black","blue"), cex = 0.8)

Exceptions <- sum(data_2005_onwards[,Return < VaR ])
cat("There are", Exceptions, "exceptions with EWMA")
```




### 1.2 

GARCH(1,1) formula:

$$\sigma^2_t = \gamma V_L + \alpha R^2_{t-1} + \beta \sigma^2_{t-1}$$ 

where $w =  \gamma V_L$

It is EWMA + long-run average

Using Built in Rugarch:

```{r}
library(rugarch)

mymodel <- ugarchspec(variance.model = list(model = "fGARCH", submodel='GARCH', garchOrder = c(1, 1)), mean.model = list(armaOrder = c(0, 0), include.mean = F), 
distribution.model = "norm")

fitVal <- ugarchfit(data =returns_data$Return , spec = mymodel, method="BFGS")
```

Start with the optimal parameters

```{r}
omega <- as.double(fitVal@fit$coef[1])
alpha1 <- as.double(fitVal@fit$coef[2])
beta1 <- as.double(fitVal@fit$coef[3])

cat("omega: ",omega,", alpha: ", alpha1, ", beta1: ", beta1, "\n")

# using the sigma from
garchVol <-fitVal@fit$sigma

returns_data <- cbind(returns_data, garchVol)

returns_data[, GARCHVaR := garchVol * z]

data_2005_onwards <- returns_data[Date >= "2015-01-01"]

plot(x = data_2005_onwards$Date, y= data_2005_onwards$Return, type = "l", 
     main="Stock Return against VaRs Garch (1,1)", xlab = "Date",
     ylab= "Stock Return")
lines(x= data_2005_onwards$Date, y= data_2005_onwards$GARCHVaR, col = "blue")
legend("bottomright",legend=c("Return","VaR Weighted"),fill=c("black","blue"), cex = 0.8)

Except <- sum(data_2005_onwards[,Return < GARCHVaR ])
cat("There are", Except, "exceptions with Garch(1,1)")
```




### 1.3 
Rerun the code from HW3 to get normalized return

```{r}
rm(list=ls())
# from
Q3_returns_data <- as.data.table(read.csv("hw3_returns2.csv"))
Q3_returns_data[,Date:=mdy(Date)]

rolling_win <- 20

# find stddev, mean
Q3_returns_data[, `:=`(volatility = shift(rollapply(Q3_returns_data$Return, rolling_win, sd ,fill=NA, align="right")), 
                       mean = shift(rollapply(Q3_returns_data$Return, 
                       rolling_win, mean ,fill=NA, align="right")))]

# normalized return
Q3_returns_data[,normalized:= (Return - mean)/volatility]

sorted_ret <- sort(Q3_returns_data$normalized)

n <- length(sorted_ret)

sorted_ret_dt <- as.data.table(sorted_ret,col=1)
sorted_ret_dt[, rank := 1]
sorted_ret_dt[, rank := cumsum(rank)]

sorted_ret_dt[, logrank := log(rank) ]
sorted_ret_dt[, logret := log(abs(sorted_ret))]

loss_dt <- sorted_ret_dt[1:(length(sorted_ret_dt$sorted_ret) * 0.05),]

fit <- lm(logrank ~ logret, data=loss_dt)
summary(fit)
plot(x= loss_dt$logret, y= loss_dt$logrank, main="Normalized left tail distribution at 5%", xlab= "log return", ylab="log rank")
abline(a= fit$coefficients[1], b=fit$coefficients[2])
```

Power law:

$$Prob(X > x) = Kx^{-1/\xi}$$

Taking log on both sides 

$$log(Prob(X > x)) = log(K) - \frac{1}{\xi}log(x)$$
In this case, the two coefficient are very significant and the regression line fits the data pretty well, indicating a strong linear relationship between log(prob) and log(loss). Hence it follows the power law.

Now, estimate the values of $\beta$ and $\xi$ using `optim` function

```{r}
sorted_ret <- sort(na.omit(Q3_returns_data$normalized))

x <-abs(sorted_ret[1:(length(sorted_ret)*0.05)])

u <- last(x)

llike <- function(x, par, u){
  xi <- par[1]
  beta <- par[2]
  n <- length(x)
  ll <- sum(log((1/beta) * (1 + xi * (x - u) / beta) ^ ((-1/xi) - 1)))
  return(-ll)
}

par <- c(0.1,0.1)
gpd.estimate <- optim(par=par, llike, x = x , u = u)

cat("estimates for xi and beta is: ", gpd.estimate$par)
xi <- gpd.estimate$par[1]
beta <- gpd.estimate$par[2]
```

Use the following formula to get VaR:

$$VaR = u + \frac{\beta}{\xi}([\frac{n}{n_u}]^{-\xi} - 1)$$

```{r}
n <- length(sorted_ret)
n_u <- n * 0.05
c <- 0.01

VaR <- u + (beta/xi) *( (n*c/n_u)^(-xi) -1)

cat("For normalized return, the VaR on the last day is: ", VaR,"\n")
```



The VaR is shown below in the normalized gain distribution.



```{r}
hist(sorted_ret,xlab= "Normalized Gains", breaks = 50
     ,col = "green", main="Normalized Gains")
abline(v = VaR * -1, col="red")
```


### 1.4

```{r}
data_2005_onwards <- Q3_returns_data[Date >= "2015-01-01"]

except_normalized <- sum(data_2005_onwards[, normalized < (VaR * -1) ])

cat("Using the VaR calculated from Pareto Distribution, the number of exceptions are: ", except_normalized,"\n")
```

There are 748 data entries from 2015 to 2017. By taking 99% VaR on these observation, we should be seeing (748 * 0.01 = 7.48) 8 exceptions. By using the pareto distribution on the normalized return, we obtained 7 observations. 

In the previous HW3, I obtained 8 exceptions using historical and exponential weighting on normalized return. I would say that pareto distribution is as a good technique to estimate the VaR as the techniques we used in HW3.





## Qn2

For question 2, I teamed up with Mu Lin. 

### 1. What was the broad trading strategy of LTCM ?

The LTCM was ran by former Solomon Brothers people who tried to replicate the similar bond arbitrage strategies and was structured to be a hedge fund, only open to large investors. As the term defined, LTCM is a hedge fund going both ways, long and short on positions which sometimes, may possess higher leverage and risk. 

The main strategy LTCM embraces is to bet on difference in yields between short-term and long-term bonds to be flatten. Specifically, the firm would long in off-the-run treasuries with higher yield and short in on-the-run treasuries with slightly lower yield; two bonds eventually converge (same maturity). The difference in yields for those closely identical bonds is due to compensation for liquidity risk. Besides, the firm also took non-arbitrage positions such as shorting equity options, emerging market debt and catastrophic bonds. 




### 2. Why did they need so much leverage?

Over a year, such trade that is long the off-the-run and short the on-the-run would be expected to return 10bp for every dollar invested. Hence, leverage is required to create attractive returns.

In general, LTCM used the tools of portfolio optimisation to structure its portfolio, leverage it by a factor of 25 to take advantage of so-called "arbitrage" trades. Due to the nature of such bond arbitrage strategy, it is somehow inevitable to impose huge leverage in order for the firm to generate attractive returns. In portfolio construction, firm set the volatility as the unleveraged position in the U.S. equity market and optimized its position with constraint in volatility and some liquidity consideration. 




### 3. How did their demise happen? 

The LTCM’s strategy can be interpreted in terms of a constrained optimization, i.e. maximizing expected returns subject to a constraint on VaR. This strategy led to the firm to its demise, as it created huge leverage and extreme sensitivity to instability in the correlations.

Beginning in May 1998, a downturn in mortgage-back securities market resulted in 16% loss to the firm, and quickly when Russia announced its restructuring plan to its government bonds, the market started to reassess credit and sovereign risk, leading to a sharp jump in credit spread while stock market dived also. Since most of the firm’s assets are borrowed (due to leverage), such sudden loss on firm’s equity increased its leverage even drastically. When market continued to deteriorate, the firm was forced to liquidate much of its assets in order to meet margin calls from derivative positions and counterparties were, leaving no choice, but to liquidate the firm’s repo collaterals in the fear that the firm could not meet further margin calls. Among 90% of losses of capital, most of it comes from bets on interest rate swap and equity volatility (out-of-money options). 




### 4. What were the most important issues with their risk management approach?

The usage of VaR as the risk measurement is heavily blamed for the failure of LTCM. As mentioned above, the firm structured its portfolio as to optimize return subject constraint in volatility and capital holdings. However, the firm assumes constant volatility as to the U.S. equity market and with its implementation of VaR in risk management, "tail risk” is gigantic, but ignored. And the fact has proven it right: when market continued to slide down and volatility spiked, tail risk became realized. Also it relied on a short time horizon to estimate the risk and assumed normal distribution on the return. Its daily VaR is set to be $409 million at 99% level in a month, however it went on a losing streak in starting from May 1997 and lost $4.4 billion during 1998. The risk measure assigned a low probability to events such as sovereignty defaults and major market disruptions, which led to huge loss when the rare events occurred.




### 5. How would you manage risk for a fund trying to trade similar strategies?

One need to understand that such trading strategy tend to induce the fund to make undiversified and highly leveraged bets and hence is fundamentally more risky. Therefore, the fund should never underestimate the risks at the tail.

A lesson to learn from the perspective of risk management, one should implement diverse measurement in risk and consider the “tail risk” to be significant when making investment decision. A time-varying GARCH model may perform better than constant normal distribution of volatility. And also one shall consider the use of expected shortfall to take into account of tail risk and tried to model the distribution of the “tail” (losses) in order to understand potential drawback. Taking a longer time horizon would also be useful to estimate a more accurate VaR by factoring in more rare events which caused huge loss in the past.



